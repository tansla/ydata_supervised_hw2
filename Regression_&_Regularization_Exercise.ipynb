{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chEqqJbLzFew"
   },
   "source": [
    "# Yandex Data Science School\n",
    "## Linear Regression & Regularization Exercise.\n",
    "\n",
    "\n",
    "## Outline\n",
    "In this exercise you will learn the following topics:\n",
    "\n",
    "1. Refresher on how linear regression is solved in batch and in Gradient Descent \n",
    "2. Implementation of Ridge Regression\n",
    "3. Comparing Ridge, Lasso and vanila Linear Regression on a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Git Exercise\n",
    "In this exercise you will also experience working with github.\n",
    "\n",
    "You might need to install local python enviroment.\n",
    "Installation Instruction for ex2 - working on a local python environment:\n",
    "https://docs.google.com/document/d/1G0rBo36ff_9JzKy0EkCalK4m_ThNUuJ2bRz463EHK9I\n",
    "\n",
    "## please add the github link of your work below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example: https://github.com/username/exercise_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/tansla/ydata-supervised-hw2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mR9UFmk2greT"
   },
   "source": [
    "## Refresher on Ordinary Least Square (OLS) aka Linear Regeression\n",
    "\n",
    "### Lecture Note\n",
    "\n",
    "In Matrix notation, the matrix $X$ is of dimensions $n \\times p$ where each row is an example and each column is a feature dimension. \n",
    "\n",
    "Similarily, $y$ is of dimension $n \\times 1$ and $w$ is of dimensions $p \\times 1$.\n",
    "\n",
    "The model is $\\hat{y}=X\\cdot w$ where we assume for simplicity that $X$'s first columns equals to 1 (one padding), to account for the bias term.\n",
    "\n",
    "Our objective is to optimize the loss $L$ defines as resiudal sum of squares (RSS): \n",
    "\n",
    "$L_{RSS}=\\frac{1}{N}\\left\\Vert Xw-y \\right\\Vert^2$ (notice that in matrix notation this means summing over all examples, so $L$ is scalar.)\n",
    "\n",
    "To find the optimal $w$ one needs to derive the loss with respect to $w$.\n",
    "\n",
    "$\\frac{\\partial{L_{RSS}}}{\\partial{w}}=\\frac{2}{N}X^T(Xw-y)$ (to see why, read about [matrix derivatives](http://www.gatsby.ucl.ac.uk/teaching/courses/sntn/sntn-2017/resources/Matrix_derivatives_cribsheet.pdf) or see class notes )\n",
    "\n",
    "Thus, the gardient descent solution is $w'=w-\\alpha \\frac{2}{N}X^T(Xw-y)$.\n",
    "\n",
    "Solving $\\frac{\\partial{L_{RSS}}}{\\partial{w}}=0$ for $w$ one can also get analytical solution:\n",
    "\n",
    "$w_{OLS}=(X^TX)^{-1}X^Ty$\n",
    "\n",
    "The first term, $(X^TX)^{-1}X^T$ is also called the pseudo inverse of $X$.\n",
    "\n",
    "See [lecture note from Stanford](https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf) for more details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JA3MEKz80vdy"
   },
   "source": [
    "## Exercise 1 - Ordinary Least Square\n",
    "* Get the boston housing dataset by using the scikit-learn package. hint: [load_boston](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html)\n",
    "\n",
    "* What is $p$? what is $n$ in the above notation? hint: [shape](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.ndarray.shape.html)\n",
    "\n",
    "* write a model `OrdinaryLinearRegression` which has a propoery $w$ and 3 methods: `fit`, `predict` and `score` (which returns the MSE on a given sample set). Hint: use [numpy.linalg.pinv](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.linalg.pinv.html) to be more efficient.\n",
    "\n",
    "* Fit the model. What is the training MSE?\n",
    "\n",
    "* Plot a scatter plot where on x-axis plot $Y$ and in the y-axis $\\hat{Y}_{OLS}$\n",
    "\n",
    "* Split the data to 75% train and 25% test 20 times. What is the average MSE now for train and test? Hint: use [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) or [ShuffleSplit](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html).\n",
    "\n",
    "* Use a t-test to proove that the MSE for training is significantly smaller than for testing. What is the p-value? Hint: use [scipy.stats.ttest_rel](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.ttest_rel.html). \n",
    "\n",
    "* Write a new class `OrdinaryLinearRegressionGradientDescent` which inherits from `OrdinaryLinearRegression` and solves the problem using gradinet descent. The class should get as a parameter the learning rate and number of iteration. Plot the class convergance. What is the effect of learning rate? How would you find number of iteration automatically? Note: Gradient Descent does not work well when features are not scaled evenly (why?!). Be sure to normalize your features first.\n",
    "\n",
    "* The following parameters are optional (not mandatory to use):\n",
    "    * early_stop - True / False boolean to indicate to stop running when loss stops decaying and False to continue.\n",
    "    * verbose- True/False boolean to turn on / off logging, e.g. print details like iteration number and loss (https://en.wikipedia.org/wiki/Verbose_mode)\n",
    "    * track_loss - True / False boolean when to save loss results to present later in learning curve graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "ZuSS8LhcfZdn"
   },
   "outputs": [],
   "source": [
    "# * write a model `Ols` which has a property $w$ and 3 methods: `fit`, `predict` and `score`.? hint: use [numpy.linalg.pinv](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.linalg.pinv.html) to be more efficient.\n",
    "\n",
    "class Ols(object):\n",
    "  def __init__(self):\n",
    "    self.w = None\n",
    "    \n",
    "  @staticmethod\n",
    "  def pad(X):\n",
    "    return np.append(np.ones([X.shape[0],1]),X,axis=1)\n",
    "  \n",
    "  def fit(self, X, Y):\n",
    "    #remember pad with 1 before fitting\n",
    "    self._fit(self.pad(X), Y)    \n",
    "  \n",
    "  def _fit(self, X, Y):\n",
    "    # optional to use this\n",
    "    self.w = np.dot(np.linalg.pinv(X), Y)\n",
    "  \n",
    "  def predict(self, X):\n",
    "    #return w\n",
    "    # optional to use this\n",
    "    return np.dot(self.pad(X) , self.w )\n",
    "    \n",
    "  def score(self, X, Y):\n",
    "    #return MSE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "X, y = load_boston(return_X_y=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.append(np.ones([X_train.shape[0],1]),X_train,axis=1)\n",
    "w = np.dot(np.linalg.pinv(X), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14,), (102, 13))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape, X_test.shape\n",
    "X_2 = np.append(np.ones([X_test.shape[0],1]),X_test,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.99672361982483"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(X_2 , w)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5.39672362,   3.62556534,   1.21694405,   2.23197915,\n",
       "         2.66987992,   3.25442929,  -0.13746182,   0.34119   ,\n",
       "         3.41320703,   3.83245597,   3.40850512,  -0.26116355,\n",
       "       -13.08842184,   0.55834668,   0.73922576,  -3.60680267,\n",
       "         1.84773313,  -4.40527282,  -9.49966034,   3.51289074,\n",
       "         2.04909479,   0.96625441,  -1.35820723,   1.76077616,\n",
       "         3.66058499,   2.03609765,   2.48148106,  -0.32295551,\n",
       "         0.73626052,   0.89631835,  -0.66616545,   1.41979081,\n",
       "        10.93909562,  -3.09837566,  -2.33088289,  -2.44874589,\n",
       "        -3.46659847,   0.63246729,  -0.63420989,   1.2322925 ,\n",
       "        -5.75795045,   4.05576301,  -7.32748839,  -0.07254954,\n",
       "         4.67618614,   1.89310991,   0.97009109,   1.70341861,\n",
       "         0.39485982,   5.95339638,   2.38860173,  -1.05548144,\n",
       "         0.84739105,  -0.27437588,  -3.96060202,   1.37950697,\n",
       "         0.44531367,   4.22531661,   1.1673037 ,  -5.61371044,\n",
       "        -2.31778092,  -4.91712243,   0.02626806,  -4.29851393,\n",
       "        -3.87905174,  -3.28910188,   7.42384893,   1.16356264,\n",
       "        -0.18392022,  -1.10216977,   0.51232627,   3.78111878,\n",
       "         0.21962332,  -4.91903656,   2.99316251,   1.19929359,\n",
       "         1.5039073 ,  -7.08622775,   7.86548695,   1.87931901,\n",
       "        -0.06923084,  -3.94379373,  -0.16036301,  -0.33517778,\n",
       "         3.82444271,   2.9155003 ,   2.70228271,   0.51579303,\n",
       "         4.04256769,   3.04358566,   2.3859787 ,  -4.69840553,\n",
       "         0.49474756,  -2.24930844, -14.99604352,  -2.27364706,\n",
       "       -25.26042839, -18.064237  ,   4.08486682,  -1.01640303,\n",
       "        -0.22378001,   3.07902364])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_ols = Ols()\n",
    "my_ols.fit(X_train,y_train)\n",
    "y_pred = my_ols.predict(X_test)\n",
    "y_pred - y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a new class OlsGd which solves the problem using gradinet descent. \n",
    "# The class should get as a parameter the learning rate and number of iteration. \n",
    "# Plot the loss convergance. for each alpha, learning rate plot the MSE with respect to number of iterations.\n",
    "# What is the effect of learning rate? \n",
    "# How would you find number of iteration automatically? \n",
    "# Note: Gradient Descent does not work well when features are not scaled evenly (why?!). Be sure to normalize your feature first.\n",
    "class Normalizer():\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def fit(self, X):\n",
    "    pass\n",
    "\n",
    "  def predict(self, X):\n",
    "    #apply normalization\n",
    "    pass\n",
    "    \n",
    "class OlsGd(Ols):\n",
    "  \n",
    "  def __init__(self, learning_rate=.05, \n",
    "               num_iteration=1000, \n",
    "               normalize=True,\n",
    "               early_stop=True,\n",
    "               verbose=True):\n",
    "    \n",
    "    super(OlsGd, self).__init__()\n",
    "    self.learning_rate = learning_rate\n",
    "    self.num_iteration = num_iteration\n",
    "    self.early_stop = early_stop\n",
    "    self.normalize = normalize\n",
    "    self.normalizer = Normalizer()    \n",
    "    self.verbose = verbose\n",
    "    \n",
    "  def _fit(self, X, Y, reset=True, track_loss=True):\n",
    "    #remeber to normalize the data before starting\n",
    "    pass\n",
    "        \n",
    "  def _predict(self, X):\n",
    "    #remeber to normalize the data before starting\n",
    "    pass\n",
    "      \n",
    "  def _step(self, X, Y):\n",
    "    # use w update for gradient descent\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HVfnXvZFi98"
   },
   "source": [
    "## Exercise 2 - Ridge Linear Regression\n",
    "\n",
    "Recall that ridge regression is identical to OLS but with a L2 penalty over the weights:\n",
    "\n",
    "$L(y,\\hat{y})=\\sum_{i=1}^{i=N}{(y^{(i)}-\\hat{y}^{(i)})^2} + \\lambda \\left\\Vert w \\right\\Vert_2^2$\n",
    "\n",
    "where $y^{(i)}$ is the **true** value and $\\hat{y}^{(i)}$ is the **predicted** value of the $i_{th}$ example, and $N$ is the number of examples\n",
    "\n",
    "* Show, by differentiating the above loss, that the analytical solution is $w_{Ridge}=(X^TX+\\lambda I)^{-1}X^Ty$\n",
    "* Change `OrdinaryLinearRegression` and `OrdinaryLinearRegressionGradientDescent` classes to work also for ridge regression (do not use the random noise analogy but use the analytical derivation). Either add a parameter, or use inheritance.\n",
    "* **Bonus: Noise as a regularizer**: Show that OLS (ordinary least square), if one adds multiplicative noise to the features the **average** solution for $W$ is equivalent to Ridge regression. In other words, if $X'= X*G$ where $G$ is an uncorrelated noise with variance $\\sigma$ and mean 1, then solving for $X'$ with OLS is like solving Ridge for $X$. What is the interpretation? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeLs(Ols):\n",
    "  def __init__(self, ridge_lambda, *wargs, **kwargs):\n",
    "    super(RidgeLs,self).__init__(*wargs, **kwargs)\n",
    "    self.ridge_lambda = ridge_lambda\n",
    "    \n",
    "  def _fit(self, X, Y):\n",
    "    #Closed form of ridge regression\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use scikitlearn implementation for OLS, Ridge and Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Regression & Regularization - Exercise.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "a6d966dc6d3bf27858c96a0cca082416ca45510cdfd9e0cf25e4971a54ae9926"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
